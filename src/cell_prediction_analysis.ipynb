{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPFL BIO 322 Project  \n",
    "### Gene Prediction based on Mouse brain single cell gene expression profiles\n",
    "\n",
    "### Authors:\n",
    "\n",
    "- Simon Lee (simon.lee@epfl.ch) \n",
    "- Léa Goffinet (lea.goffinet@epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "In an experiment on epigenetics and memory, Giulia Santoni (from the lab of Johannes Gräff at\n",
    "EPFL) measured the gene expression levels in multiple cells of a mouse brain under three different\n",
    "conditions that we call KAT5, CBP and eGFP. In this challenge, the goal is to predict – as accurately\n",
    "as possible – for each cell the experimental condition (KAT5, CBP or eGFP) under which it was\n",
    "measured, given only the gene expression levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process\n",
    "import concurrent\n",
    "from multiprocessing import Pool\n",
    "import xgboost as xgb\n",
    "import glob\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and visualisation\n",
    "\n",
    "This section covers the loading, filtering and visualisation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw data\n",
    "df = pd.read_csv('../data/train.csv.gz', compression='gzip')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected single cell data is extremely sparse. Lets count to see how many non zero entries we actually have per column to get a glimpse of what might be important genes and what might not be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_column_headers = df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this cell only if you want to do filter and write out the counts of how many non zero columns are in each column to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this gets the counts of each column and drops the column accordingly\n",
    "# f = open('../data/counts.txt', 'w')\n",
    "# for gene in gene_column_headers:\n",
    "#     count = (df[gene] != 0).sum()\n",
    "    \n",
    "#     f.write('Counts of gene in cells {} : {} \\n'.format(gene, count))  # Uncomment if you want to generate text file with counts\n",
    "\n",
    "#     # new data generator: Takes 3 hrs to run!!\n",
    "#     # if count < 500:\n",
    "#     #     df = df.drop(columns=[gene])\n",
    "\n",
    "# # df.to_csv('../data/filtered_train.csv.gz, compression='gzip')\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv('../data/filtered_train.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the filtering process removed over 22,000 genes that were seen across less than 10% of the cells. Though the threshold is a hyperparameter we believe choosing a smaller hyperparameter will be a safer bet to not throw away useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any null values incase we need to perform imputation\n",
    "check_nan = df.isnull().values.any()\n",
    "print(check_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_column_headers = df_filtered.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're performing PCA, once with the whole data and once with the filtered data, to see if there is different clusters but also to make sure that the filtered data is corresponding to the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = df.iloc[:, :-1].values\n",
    "y = df.loc[:,['labels']].values\n",
    "\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "df_pca = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_final = pd.concat([df_pca, df[['labels']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "labels = ['CBP', 'KAT5', 'eGFP']\n",
    "colors = ['r', 'g', 'b']\n",
    "for label, color in zip(labels,colors):\n",
    "    indicesToKeep = df_pca_final['labels'] == label\n",
    "    plt.scatter(df_pca_final.loc[indicesToKeep, 'principal component 1'], df_pca_final.loc[indicesToKeep, 'principal component 2'], c = color, s = 25)\n",
    "plt.legend(labels)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA : 2 components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n",
    "#very low explained variance with 2 components..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same with df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_filtered.iloc[:, :-1].values\n",
    "y = df_filtered.loc[:,['labels']].values\n",
    "\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "df_pca = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "df_pca_final = pd.concat([df_pca, df[['labels']]], axis = 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "labels = ['CBP', 'KAT5', 'eGFP']\n",
    "colors = ['r', 'g', 'b']\n",
    "for label, color in zip(labels,colors):\n",
    "    indicesToKeep = df_pca_final['labels'] == label\n",
    "    plt.scatter(df_pca_final.loc[indicesToKeep, 'principal component 1'], df_pca_final.loc[indicesToKeep, 'principal component 2'], c = color, s = 25)\n",
    "plt.legend(labels)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA : 2 components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.iloc[:,:-1].values\n",
    "y = df_filtered['labels'].values\n",
    "\n",
    "X = StandardScaler().fit_transform(x)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = LogisticRegression(multi_class='multinomial',solver ='saga')\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalized=True, cmap='bone'):\n",
    "    norm_cm = cm\n",
    "    if normalized:\n",
    "        norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=classes, yticklabels=classes, cmap=cmap)\n",
    "\n",
    "plot_confusion_matrix(cm, ['KAT5', 'CBP', 'eGFP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Multiple linear regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mmmmm Logistic is so good, I'm not sure it's worth it to let multiple linear regression, ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.iloc[:,:-1]\n",
    "y = df_filtered['labels']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y)\n",
    "  \n",
    "# Splitting the data into training and testing data\n",
    "model = LinearRegression()\n",
    "  \n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm = confusion_matrix(y_test, y_pred)\n",
    "#plot_confusion_matrix(cm, ['KAT5', 'CBP', 'eGFP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = pd.DataFrame()\n",
    "coeffs['gene'] = X_train.columns\n",
    "coeffs['coeffcient'] = pd.Series(model.coef_)\n",
    "coeffs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error_ridge = np.mean((y_pred - y_test)**2)\n",
    "print(mean_squared_error_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = 1)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred1 = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error = np.mean((y_pred1 - y_test)**2)\n",
    "print(\"Mean squared error on test set\", mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coeff = pd.DataFrame()\n",
    "lasso_coeff['gene_column_headers'] = X_train.columns\n",
    "lasso_coeff['coefficient'] = pd.Series(lasso.coef_)\n",
    " \n",
    "lasso_coeff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 XGBoost\n",
    "\n",
    "- Once with our filtered data\n",
    "- Also run with raw data to see if filtering has an effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and validation set using sklearn\n",
    "gene_column_headers_filtered = gene_column_headers[:-1]\n",
    "y = df_filtered['labels']\n",
    "X = df_filtered.iloc[:,:-1]\n",
    "\n",
    "# need to transform our labels from [KAT5, CBP, eGFP] -> [0,1,2] \n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a typical split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify = y)\n",
    "\n",
    "training_data = {'X_train':X_train,'y_train':y_train,\n",
    "                'X_test': X_val,'y_test':y_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit function where it takes the sklearn and xgboost models and performs the boosted trees.\n",
    "# plots performance and accuracy as well\n",
    "\n",
    "def fit(model, training_data=training_data, epochs=300, label_gene = gene_column_headers_filtered):\n",
    "    # fitting to the sklearn model\n",
    "    print('Fitting model...')\n",
    "    model.fit(training_data['X_train'], training_data['y_train'])\n",
    "    print('Fitting done!')\n",
    "\n",
    "    # fitting the xboost library model\n",
    "    train = xgb.DMatrix(training_data['X_train'], label=training_data['y_train'])\n",
    "    test = xgb.DMatrix(training_data['X_test'], label=training_data['y_test'])\n",
    "    params = model.get_xgb_params()\n",
    "    metrics = ['mlogloss','merror']\n",
    "    params['eval_metric'] = metrics\n",
    "    evaluation = {}\n",
    "    evallist = [(test, 'test'),(train,'train')]\n",
    "    xgb_model = xgb.train(params, train, epochs, evallist, evals_result=evaluation,verbose_eval=100)\n",
    "\n",
    "    # Model reports\n",
    "    print('-- Model Report --')\n",
    "    print('XGBoost Accuracy: '+str(accuracy_score(model.predict(training_data['X_test']), training_data['y_test'])))\n",
    "    print('XGBoost F1-Score: '+str(f1_score(model.predict(training_data['X_test']),training_data['y_test'], average='micro')))\n",
    "    \n",
    "    # plotting the error curves for our loss functions\n",
    "    for m in metrics:\n",
    "        test_score = evaluation['test'][m]\n",
    "        train_score = evaluation['train'][m]\n",
    "        x = range(0, epochs)\n",
    "        plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "        plt.plot(x, test_score, label=\"Test\")\n",
    "        plt.plot(x, train_score, label=\"Train\")\n",
    "        \n",
    "        title_name = m + \" plot\"\n",
    "        plt.title(title_name)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(m)\n",
    "        lgd = plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # makes sure that the two array match so we can plot the feature importance\n",
    "    print(\"length of features list: {}\".format(len(gene_column_headers_filtered)))\n",
    "    print(\"length of feature importance vector {}\".format(len(model.feature_importances_)))\n",
    "\n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['X_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters that can be adjusted in the final model\n",
    "xgb_model = XGBClassifier(learning_rate=0.1, # play around with learning rate\n",
    "                    n_estimators=300, # play around with number of boosted trees built\n",
    "                    max_depth=9, # play around with tree depth\n",
    "                    objective='multi:softmax',  # I saw that using softmax or softprob is best for multi class classification\n",
    "                    nthread=4,\n",
    "                    num_class=3,\n",
    "                    seed=1 # seed is included for reproducibility\n",
    "                    )\n",
    "\n",
    "xgb_trained = fit(xgb_model, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in test.csv.gz to assess model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv.gz', compression='gzip', usecols=gene_column_headers_filtered)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get it into dmatrix again\n",
    "X = test\n",
    "testing_data = xgb.DMatrix(data=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xgb.DMatrix(training_data['X_test'], label=training_data['y_test'])\n",
    "pred = xgb_trained.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(training_data['y_test'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(training_data['y_test'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "plt.rcParams[\"figure.figsize\"] = [22,40]\n",
    "xgb.plot_tree(xgb_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [22,22]\n",
    "xgb.plot_importance(xgb_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as expected alot of features aren't very important but lets still try running our classifier with our raw data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data. Assess performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and validation set using sklearn\n",
    "y = df['labels']\n",
    "X = df.loc[:, df.columns != 'labels'] \n",
    "\n",
    "# need to transform our labels from [KAT5, CBP, eGFP] -> [0,1,2] \n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a typical split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y)\n",
    "\n",
    "data = {'X_train':X_train,'y_train':y_train,\n",
    "                'X_test': X_val,'y_test':y_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters that can be adjusted in the final model\n",
    "xgb_model = XGBClassifier(learning_rate=0.1, # play around with learning rate\n",
    "                    n_estimators=300, # play around with number of boosted trees built\n",
    "                    max_depth=5, # play around with tree depth\n",
    "                    objective='multi:softmax',  # I saw that using softmax or softprob is best for multi class classification\n",
    "                    nthread=4,\n",
    "                    num_class=3,\n",
    "                    seed=1 # seed is included for reproducibility\n",
    "                    )\n",
    "\n",
    "xgb_trained = fit(xgb_model, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I kinda 'cleaned' the code a bit, but I don't know if you wanted to include multiple models with different hyperparamaters, cause maybe it's enough to mention in the report that we played around with, or do we let them in ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9c10a8121bbc77dcf30429ad2ceb1cec05622d2392b89908360caf11e90a899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
